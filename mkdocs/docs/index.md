# MLPerf Training BERT Pytorch


Hello there, time-travelers from the future! Welcome to the exciting chronicle of our BERT pretraining journey. If you're reading this a few months or even a few years from now, we hope this document brings back fond memories of the thrilling adventure we embarked on together. 

BERT, or Bidirectional Encoder Representations from Transformers, is our trusty companion in this quest. It's a method of pretraining language representations that makes our models more knowledgeable about language context and sentence structure. But you already knew that, didn't you? 

Our journey is filled with the thrill of setting up environments on different GPUs, the suspense of model convergence, and the triumph of overcoming challenges. We've meticulously documented our steps, findings, and solutions to ensure that this document serves as a reliable guide for our future selves. 

We've navigated the vast seas of TensorFlow and PyTorch, delved into the depths of Docker containers, and scaled the heights of various hardware configurations. We've left no stone unturned in our pursuit of the most efficient BERT pretraining setup.

Buckle up and get ready for a trip down memory lane! Whether you're here to reminisce or to troubleshoot, we've got you covered. This document is more than just a log of our work; it's a testament to our dedication, perseverance, and the fun we had along the way. 

Here's to the past, and to the future! Let's dive in!

## Further info

Please contact <a vartag='ANTON_EMAIL'>anton@krai.ai</a>.